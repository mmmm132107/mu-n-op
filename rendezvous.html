<!DOCTYPE html>
<html lang="vi">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>Voice Debug ‚Äî rendezvous</title>

<style>
body{
  margin:0;
  background:#05070c;
  color:#f3e8dc;
  font-family:system-ui,-apple-system,Segoe UI;
}

.wrap{
  max-width:900px;
  margin:60px auto;
  padding:0 18px;
}

button{
  padding:12px 18px;
  border-radius:12px;
  border:1px solid #d8c8b8;
  background:#16161c;
  color:#f3e8dc;
  cursor:pointer;
}
button:hover{transform:scale(1.02);}

#status{
  margin-top:14px;
  padding:10px 12px;
  border-radius:10px;
  background:#111217;
  min-height:44px;
  font-size:14px;
  line-height:1.6;
  white-space:pre-line;
}

.log{
  margin-top:18px;
  padding:12px;
  border-radius:10px;
  background:#0c0d13;
  font-family:monospace;
  font-size:13px;
  max-height:220px;
  overflow-y:auto;
}
</style>
</head>

<body>

<div class="wrap">

<h2>Ki·ªÉm tra web c√≥ nh·∫≠n gi·ªçng n√≥i hay kh√¥ng</h2>

<button id="listenBtn">B·∫•m ƒë·ªÉ n√≥i</button>

<div id="status"></div>

<div class="log" id="logBox"></div>

</div>

<script>
const statusBox = document.getElementById("status");
const logBox = document.getElementById("logBox");
const listenBtn = document.getElementById("listenBtn");

function log(msg){
  console.log(msg);
  logBox.textContent += msg + "\n";
  logBox.scrollTop = logBox.scrollHeight;
}

/* ========== KH·ªûI T·∫†O SPEECH API ========== */

const SpeechRecognition =
  window.SpeechRecognition || window.webkitSpeechRecognition;

if(!SpeechRecognition){
  statusBox.textContent =
    "‚ö† Tr√¨nh duy·ªát kh√¥ng h·ªó tr·ª£ SpeechRecognition.\n" +
    "H√£y th·ª≠ b·∫±ng Chrome ho·∫∑c Edge.";
  log("SpeechRecognition NOT supported");
} else {
  log("SpeechRecognition supported ‚úì");
}

const rec = new SpeechRecognition();
rec.lang = "vi-VN";
rec.interimResults = false;

/* ========== EVENT DEBUG ========== */

rec.onstart = () => {
  statusBox.textContent = "üé§ ƒê√£ b·∫Øt ƒë·∫ßu nghe‚Ä¶ h√£y n√≥i v√†o mic";
  log("onstart ‚úì recognition started");
};

rec.onaudiostart = () => log("onaudiostart ‚úì mic nh·∫≠n √¢m thanh");
rec.onspeechstart = () => log("onspeechstart ‚úì ph√°t hi·ªán gi·ªçng n√≥i");
rec.onspeechend = () => log("onspeechend ‚Äî ng·ª´ng n√≥i");
rec.onaudioend = () => log("onaudioend ‚Äî k·∫øt th√∫c audio");
rec.onend = () => log("onend ‚Äî recognition session ended");

rec.onnomatch = () => {
  statusBox.textContent = "‚ùì Kh√¥ng nghe r√µ ho·∫∑c kh√¥ng hi·ªÉu c√¢u n√≥i";
  log("onnomatch ‚Äî no speech recognized");
};

rec.onerror = (e) => {
  statusBox.textContent = "‚õî L·ªói: " + e.error;
  log("ERROR ‚Üí " + e.error);
};

/* ========== KHI NH·∫¨N ƒê∆Ø·ª¢C K·∫æT QU·∫¢ ========== */

rec.onresult = (e) => {

  const spoken = e.results[0][0].transcript.trim();
  const conf = e.results[0][0].confidence;

  statusBox.textContent =
    "Web nghe ƒë∆∞·ª£c:\n" +
    `"${spoken}"\n` +
    `(ƒë·ªô tin c·∫≠y: ${(conf*100).toFixed(1)}%)`;

  log(`RESULT = "${spoken}"`);
};

/* ========== N√öT B·∫ÆT ƒê·∫¶U NGHE ========== */

listenBtn.onclick = () => {
  log("\n\n=== CLICK: START LISTENING ===");
  statusBox.textContent = "‚è≥ ƒêang kh·ªüi ƒë·ªông microphone‚Ä¶";
  rec.start();
};
</script>

</body>
</html>

